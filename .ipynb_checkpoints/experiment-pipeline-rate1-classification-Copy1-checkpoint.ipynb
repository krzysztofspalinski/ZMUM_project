{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './GoogleNews-vectors-negative300.bin.gz' \n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krzysztof/.conda/envs/ml-nlp/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/Train.csv', sep=';')\n",
    "test = pd.read_csv('./data/TestX.csv', sep=';')\n",
    "\n",
    "train.opinion = train.opinion.apply(lambda x: x.replace(\"&#039;\", \"'\"))\n",
    "test.opinion = test.opinion.apply(lambda x: x.replace(\"&#039;\", \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates('opinion').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 30000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['opinion_length'] = train.opinion.apply(lambda x: len(x))\n",
    "train['capital_counts'] = train.opinion.apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "train['special_counts'] = train.opinion.apply(lambda x: sum(1 for c in x if c in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~'))\n",
    "train['word_count'] = train.opinion.apply(lambda x: len(str(x).split()))\n",
    "train['unique_word_count'] = train.opinion.apply(lambda x: len(set(str(x).split())))\n",
    "train['mean_word_length'] = train.opinion.apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "train['name_count'] = train.opinion.apply(lambda x: sum(1 for c in x.split() if c[0].isupper()))\n",
    "\n",
    "test['opinion_length'] = test.opinion.apply(lambda x: len(x))\n",
    "test['capital_counts'] = test.opinion.apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "test['special_counts'] = test.opinion.apply(lambda x: sum(1 for c in x if c in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~'))\n",
    "test['word_count'] = test.opinion.apply(lambda x: len(str(x).split()))\n",
    "test['unique_word_count'] = test.opinion.apply(lambda x: len(set(str(x).split())))\n",
    "test['mean_word_length'] = test.opinion.apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test['name_count'] = test.opinion.apply(lambda x: sum(1 for c in x.split() if c[0].isupper()))\n",
    "\n",
    "meta_features = ['opinion_length', 'capital_counts', 'special_counts', 'word_count', 'unique_word_count', 'mean_word_length', 'name_count']\n",
    "\n",
    "for feature in meta_features:\n",
    "    train[feature] = train[feature] / train[feature].max()\n",
    "    test[feature] = test[feature] / train[feature].max()\n",
    "\n",
    "condition_features = list(train.condition.value_counts().index)[:60]\n",
    "for condition_feature in condition_features:\n",
    "    train['condition_' + condition_feature] = (train.condition == condition_feature).astype(int)\n",
    "    test['condition_' + condition_feature] = (test.condition == condition_feature).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "import re\n",
    "\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "\n",
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'}\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107590/107590 [00:00<00:00, 152085.59it/s]\n",
      "100%|██████████| 107590/107590 [00:01<00:00, 81932.54it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = train[\"opinion\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188722/188722 [00:00<00:00, 573826.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 20.39% of vocab\n",
      "Found embeddings for  77.62% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['opinion'] = train['opinion'].str.replace('\"', '')\n",
    "train['opinion'] = train['opinion'].str.replace(',', '')\n",
    "train['opinion'] = train['opinion'].str.replace('.', '')\n",
    "train['opinion'] = train['opinion'].str.replace('!', '')\n",
    "train['opinion'] = train['opinion'].str.replace(':', '')\n",
    "train['opinion'] = train['opinion'].str.replace('&#039;ve', ' have')\n",
    "train['opinion'] = train['opinion'].str.replace('&#039;s', ' is')\n",
    "train['opinion'] = train['opinion'].str.replace('&#039;t', ' not')\n",
    "train['opinion'] = train['opinion'].str.replace('&#039;m', ' am')\n",
    "train['opinion'] = train['opinion'].str.replace('&#039;ll', '')\n",
    "train['opinion'] = train['opinion'].str.replace('rsquot', '')\n",
    "train['opinion'] = train['opinion'].str.replace('rsquom', '')\n",
    "train['opinion'] = train['opinion'].str.replace('rsquos', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['opinion'] = test['opinion'].str.replace('\"', '')\n",
    "test['opinion'] = test['opinion'].str.replace(',', '')\n",
    "test['opinion'] = test['opinion'].str.replace('.', '')\n",
    "test['opinion'] = test['opinion'].str.replace('!', '')\n",
    "test['opinion'] = test['opinion'].str.replace(':', '')\n",
    "test['opinion'] = test['opinion'].str.replace('&#039;ve', ' have')\n",
    "test['opinion'] = test['opinion'].str.replace('&#039;s', ' is')\n",
    "test['opinion'] = test['opinion'].str.replace('&#039;t', ' not')\n",
    "test['opinion'] = test['opinion'].str.replace('&#039;m', ' am')\n",
    "test['opinion'] = test['opinion'].str.replace('&#039;ll', '')\n",
    "test['opinion'] = test['opinion'].str.replace('rsquot', '')\n",
    "test['opinion'] = test['opinion'].str.replace('rsquom', '')\n",
    "test['opinion'] = test['opinion'].str.replace('rsquos', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107590/107590 [00:00<00:00, 119520.18it/s]\n",
      "100%|██████████| 107590/107590 [00:03<00:00, 32668.89it/s]\n",
      "100%|██████████| 107590/107590 [00:01<00:00, 83091.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"opinion\"] = train[\"opinion\"].progress_apply(lambda x: clean_text(x))\n",
    "train[\"opinion\"] = train[\"opinion\"].progress_apply(lambda x: clean_numbers(x))\n",
    "train[\"opinion\"] = train[\"opinion\"].progress_apply(lambda x: replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 118100.24it/s]\n",
      "100%|██████████| 50000/50000 [00:01<00:00, 33032.14it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 82098.76it/s]\n"
     ]
    }
   ],
   "source": [
    "test[\"opinion\"] = test[\"opinion\"].progress_apply(lambda x: clean_text(x))\n",
    "test[\"opinion\"] = test[\"opinion\"].progress_apply(lambda x: clean_numbers(x))\n",
    "test[\"opinion\"] = test[\"opinion\"].progress_apply(lambda x: replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107590/107590 [00:01<00:00, 85361.04it/s]\n",
      "100%|██████████| 107590/107590 [00:00<00:00, 122262.64it/s]\n",
      "100%|██████████| 107590/107590 [00:01<00:00, 98968.87it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = train[\"opinion\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74690/74690 [00:00<00:00, 543592.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 57.90% of vocab\n",
      "Found embeddings for  99.12% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nexplanon', 1254),\n",
       " ('Sprintec', 824),\n",
       " ('nexplanon', 744),\n",
       " ('rsquot', 673),\n",
       " ('Belviq', 585),\n",
       " ('mirena', 445),\n",
       " ('rsquom', 442),\n",
       " ('skyla', 399),\n",
       " ('rsquos', 382),\n",
       " ('implanon', 366)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_oov = [i for i, j in oov[:40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for oov_word in top_oov:\n",
    "    train['oov_' + oov_word] = train.opinion.apply(lambda x: oov_word in x)\n",
    "    test['oov_' + oov_word] = test.opinion.apply(lambda x: oov_word in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = meta_features + ['condition_' + i for i in condition_features] + ['oov_' + i for i in top_oov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(train, test_size=0.08, random_state=42)\n",
    "\n",
    "X_train_meta = np.array(train_df.loc[:, features]).astype(np.float32)\n",
    "X_val_meta = np.array(val_df.loc[:, features]).astype(np.float32)\n",
    "X_test_meta = np.array(test.loc[:, features]).astype(np.float32)\n",
    "\n",
    "X_train = train_df.opinion.values\n",
    "X_val = val_df.opinion.values\n",
    "X_test = test.opinion.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['rate1'].values\n",
    "y_val = val_df['rate1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, GlobalMaxPool1D, Bidirectional, LSTM, Embedding, Input, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((max_features, embed_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word2vec.get_vector(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embedding_vector = None\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['high' 'low' 'medium']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "print(le.classes_)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_models_cls import model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Model 1, iteration 0\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 27s 288us/sample - loss: 0.7305 - accuracy: 0.6939 - val_loss: 0.6388 - val_accuracy: 0.7354\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 25s 266us/sample - loss: 0.5873 - accuracy: 0.7560 - val_loss: 0.6029 - val_accuracy: 0.7497\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 25s 267us/sample - loss: 0.5389 - accuracy: 0.7774 - val_loss: 0.5713 - val_accuracy: 0.7624\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 25s 267us/sample - loss: 0.5012 - accuracy: 0.7931 - val_loss: 0.5729 - val_accuracy: 0.7651\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 25s 268us/sample - loss: 0.4647 - accuracy: 0.8091 - val_loss: 0.5790 - val_accuracy: 0.7614\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 25s 267us/sample - loss: 0.4233 - accuracy: 0.8278 - val_loss: 0.6090 - val_accuracy: 0.7541\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 25s 267us/sample - loss: 0.3876 - accuracy: 0.8440 - val_loss: 0.6359 - val_accuracy: 0.7394\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 25s 268us/sample - loss: 0.3422 - accuracy: 0.8634 - val_loss: 0.6933 - val_accuracy: 0.7412\n",
      "##################################################\n",
      "Model 1, iteration 1\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 28s 296us/sample - loss: 0.7405 - accuracy: 0.6884 - val_loss: 0.6392 - val_accuracy: 0.7309\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 26s 279us/sample - loss: 0.5892 - accuracy: 0.7542 - val_loss: 0.5713 - val_accuracy: 0.7648\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 26s 278us/sample - loss: 0.5406 - accuracy: 0.7761 - val_loss: 0.5980 - val_accuracy: 0.7483\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 26s 279us/sample - loss: 0.5031 - accuracy: 0.7917 - val_loss: 0.5577 - val_accuracy: 0.7737\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 26s 278us/sample - loss: 0.4667 - accuracy: 0.8076 - val_loss: 0.5573 - val_accuracy: 0.7770\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 26s 278us/sample - loss: 0.4253 - accuracy: 0.8277 - val_loss: 0.5925 - val_accuracy: 0.7610\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 26s 280us/sample - loss: 0.3902 - accuracy: 0.8429 - val_loss: 0.6147 - val_accuracy: 0.7549\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 26s 279us/sample - loss: 0.3479 - accuracy: 0.8607 - val_loss: 0.6547 - val_accuracy: 0.7673\n",
      "Epoch 9/20\n",
      "94032/94032 [==============================] - 26s 279us/sample - loss: 0.3015 - accuracy: 0.8813 - val_loss: 0.7122 - val_accuracy: 0.7503\n",
      "Epoch 10/20\n",
      "94032/94032 [==============================] - 27s 289us/sample - loss: 0.2621 - accuracy: 0.8982 - val_loss: 0.7894 - val_accuracy: 0.7545\n",
      "##################################################\n",
      "Model 1, iteration 2\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 25s 267us/sample - loss: 0.7466 - accuracy: 0.6851 - val_loss: 0.6547 - val_accuracy: 0.7228\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 24s 250us/sample - loss: 0.5997 - accuracy: 0.7511 - val_loss: 0.5952 - val_accuracy: 0.7537\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 26s 272us/sample - loss: 0.5496 - accuracy: 0.7726 - val_loss: 0.6243 - val_accuracy: 0.7422\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 25s 265us/sample - loss: 0.5137 - accuracy: 0.7875 - val_loss: 0.5812 - val_accuracy: 0.7533\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 25s 264us/sample - loss: 0.4776 - accuracy: 0.8036 - val_loss: 0.5850 - val_accuracy: 0.7598\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 25s 266us/sample - loss: 0.4398 - accuracy: 0.8215 - val_loss: 0.6131 - val_accuracy: 0.7576\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 25s 266us/sample - loss: 0.3998 - accuracy: 0.8382 - val_loss: 0.6340 - val_accuracy: 0.7511\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 25s 267us/sample - loss: 0.3587 - accuracy: 0.8567 - val_loss: 0.7428 - val_accuracy: 0.7135\n",
      "Epoch 9/20\n",
      "94032/94032 [==============================] - 25s 267us/sample - loss: 0.3129 - accuracy: 0.8777 - val_loss: 0.7465 - val_accuracy: 0.7335\n",
      "##################################################\n",
      "Model 1, iteration 3\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 27s 286us/sample - loss: 0.7341 - accuracy: 0.6896 - val_loss: 0.6449 - val_accuracy: 0.7285\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 25s 267us/sample - loss: 0.5899 - accuracy: 0.7554 - val_loss: 0.5939 - val_accuracy: 0.7507\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 25s 268us/sample - loss: 0.5391 - accuracy: 0.7775 - val_loss: 0.5950 - val_accuracy: 0.7529\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 25s 269us/sample - loss: 0.5028 - accuracy: 0.7921 - val_loss: 0.5845 - val_accuracy: 0.7533\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 25s 261us/sample - loss: 0.4647 - accuracy: 0.8094 - val_loss: 0.6091 - val_accuracy: 0.7545\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 25s 261us/sample - loss: 0.4302 - accuracy: 0.8250 - val_loss: 0.6060 - val_accuracy: 0.7503\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 25s 261us/sample - loss: 0.3889 - accuracy: 0.8426 - val_loss: 0.6314 - val_accuracy: 0.7517\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 25s 262us/sample - loss: 0.3499 - accuracy: 0.8593 - val_loss: 0.6660 - val_accuracy: 0.7432\n",
      "Epoch 9/20\n",
      "94032/94032 [==============================] - 25s 262us/sample - loss: 0.3050 - accuracy: 0.8790 - val_loss: 0.7555 - val_accuracy: 0.7329\n",
      "##################################################\n",
      "Model 1, iteration 4\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 26s 281us/sample - loss: 0.7353 - accuracy: 0.6891 - val_loss: 0.6638 - val_accuracy: 0.7119\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 25s 264us/sample - loss: 0.5945 - accuracy: 0.7519 - val_loss: 0.5658 - val_accuracy: 0.7622\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 25s 263us/sample - loss: 0.5459 - accuracy: 0.7747 - val_loss: 0.6037 - val_accuracy: 0.7479\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 25s 262us/sample - loss: 0.5092 - accuracy: 0.7893 - val_loss: 0.5713 - val_accuracy: 0.7618\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 25s 263us/sample - loss: 0.4736 - accuracy: 0.8049 - val_loss: 0.5534 - val_accuracy: 0.7707\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 25s 263us/sample - loss: 0.4357 - accuracy: 0.8210 - val_loss: 0.5762 - val_accuracy: 0.7628\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 25s 264us/sample - loss: 0.3978 - accuracy: 0.8384 - val_loss: 0.6150 - val_accuracy: 0.7499\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 25s 264us/sample - loss: 0.3570 - accuracy: 0.8574 - val_loss: 0.6754 - val_accuracy: 0.7582\n",
      "Epoch 9/20\n",
      "94032/94032 [==============================] - 25s 270us/sample - loss: 0.3143 - accuracy: 0.8757 - val_loss: 0.7405 - val_accuracy: 0.7424\n",
      "Epoch 10/20\n",
      "94032/94032 [==============================] - 23s 244us/sample - loss: 0.2735 - accuracy: 0.8933 - val_loss: 0.8151 - val_accuracy: 0.7493\n",
      "##################################################\n",
      "Model 1, iteration 5\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 24s 260us/sample - loss: 0.7276 - accuracy: 0.6938 - val_loss: 0.6427 - val_accuracy: 0.7347\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 23s 243us/sample - loss: 0.5914 - accuracy: 0.7531 - val_loss: 0.5979 - val_accuracy: 0.7523\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 23s 243us/sample - loss: 0.5415 - accuracy: 0.7768 - val_loss: 0.5855 - val_accuracy: 0.7556\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5014 - accuracy: 0.7934 - val_loss: 0.5924 - val_accuracy: 0.7622\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.4648 - accuracy: 0.8096 - val_loss: 0.6099 - val_accuracy: 0.7436\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.4232 - accuracy: 0.8272 - val_loss: 0.6292 - val_accuracy: 0.7566\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.3852 - accuracy: 0.8442 - val_loss: 0.6563 - val_accuracy: 0.7386\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.3429 - accuracy: 0.8625 - val_loss: 0.7566 - val_accuracy: 0.7311\n",
      "##################################################\n",
      "Model 1, iteration 6\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 24s 260us/sample - loss: 0.7296 - accuracy: 0.6919 - val_loss: 0.6463 - val_accuracy: 0.7240\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5933 - accuracy: 0.7526 - val_loss: 0.6442 - val_accuracy: 0.7242\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5417 - accuracy: 0.7753 - val_loss: 0.5960 - val_accuracy: 0.7521\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.5026 - accuracy: 0.7928 - val_loss: 0.5933 - val_accuracy: 0.7574\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.4671 - accuracy: 0.8089 - val_loss: 0.6113 - val_accuracy: 0.7501\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.4335 - accuracy: 0.8229 - val_loss: 0.6339 - val_accuracy: 0.7483\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 23s 240us/sample - loss: 0.3958 - accuracy: 0.8395 - val_loss: 0.6746 - val_accuracy: 0.7337\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.3519 - accuracy: 0.8596 - val_loss: 0.7034 - val_accuracy: 0.7396\n",
      "Epoch 9/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.3141 - accuracy: 0.8768 - val_loss: 0.7652 - val_accuracy: 0.7418\n",
      "##################################################\n",
      "Model 1, iteration 7\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 24s 259us/sample - loss: 0.7151 - accuracy: 0.6986 - val_loss: 0.6288 - val_accuracy: 0.7341\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5841 - accuracy: 0.7576 - val_loss: 0.6098 - val_accuracy: 0.7410\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5388 - accuracy: 0.7771 - val_loss: 0.6057 - val_accuracy: 0.7481\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.4990 - accuracy: 0.7937 - val_loss: 0.5896 - val_accuracy: 0.7515\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.4631 - accuracy: 0.8095 - val_loss: 0.6224 - val_accuracy: 0.7368\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.4261 - accuracy: 0.8254 - val_loss: 0.6082 - val_accuracy: 0.7465\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.3834 - accuracy: 0.8462 - val_loss: 0.6234 - val_accuracy: 0.7457\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.3519 - accuracy: 0.8573 - val_loss: 0.6715 - val_accuracy: 0.7451\n",
      "Epoch 9/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.3070 - accuracy: 0.8785 - val_loss: 0.7473 - val_accuracy: 0.7507\n",
      "##################################################\n",
      "Model 1, iteration 8\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 24s 259us/sample - loss: 0.7198 - accuracy: 0.6964 - val_loss: 0.6246 - val_accuracy: 0.7422\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5863 - accuracy: 0.7558 - val_loss: 0.6261 - val_accuracy: 0.7323\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5381 - accuracy: 0.7769 - val_loss: 0.5810 - val_accuracy: 0.7574\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.4975 - accuracy: 0.7941 - val_loss: 0.5845 - val_accuracy: 0.7602\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 23s 240us/sample - loss: 0.4583 - accuracy: 0.8110 - val_loss: 0.5803 - val_accuracy: 0.7584\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.4174 - accuracy: 0.8303 - val_loss: 0.6299 - val_accuracy: 0.7410\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.3793 - accuracy: 0.8459 - val_loss: 0.6402 - val_accuracy: 0.7477\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 23s 240us/sample - loss: 0.3305 - accuracy: 0.8682 - val_loss: 0.7020 - val_accuracy: 0.7436\n",
      "Epoch 9/20\n",
      "94032/94032 [==============================] - 23s 242us/sample - loss: 0.2921 - accuracy: 0.8854 - val_loss: 0.7732 - val_accuracy: 0.7216\n",
      "Epoch 10/20\n",
      "94032/94032 [==============================] - 23s 240us/sample - loss: 0.2569 - accuracy: 0.8996 - val_loss: 0.8449 - val_accuracy: 0.7428\n",
      "##################################################\n",
      "Model 1, iteration 9\n",
      "Train on 94032 samples, validate on 4950 samples\n",
      "Epoch 1/20\n",
      "94032/94032 [==============================] - 24s 258us/sample - loss: 0.7302 - accuracy: 0.6925 - val_loss: 0.6549 - val_accuracy: 0.7285\n",
      "Epoch 2/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5870 - accuracy: 0.7555 - val_loss: 0.5905 - val_accuracy: 0.7539\n",
      "Epoch 3/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.5402 - accuracy: 0.7760 - val_loss: 0.5777 - val_accuracy: 0.7646\n",
      "Epoch 4/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.4985 - accuracy: 0.7935 - val_loss: 0.5793 - val_accuracy: 0.7606\n",
      "Epoch 5/20\n",
      "94032/94032 [==============================] - 23s 240us/sample - loss: 0.4632 - accuracy: 0.8100 - val_loss: 0.5866 - val_accuracy: 0.7560\n",
      "Epoch 6/20\n",
      "94032/94032 [==============================] - 23s 240us/sample - loss: 0.4202 - accuracy: 0.8293 - val_loss: 0.6189 - val_accuracy: 0.7503\n",
      "Epoch 7/20\n",
      "94032/94032 [==============================] - 23s 240us/sample - loss: 0.3779 - accuracy: 0.8476 - val_loss: 0.6723 - val_accuracy: 0.7521\n",
      "Epoch 8/20\n",
      "94032/94032 [==============================] - 23s 241us/sample - loss: 0.3326 - accuracy: 0.8669 - val_loss: 0.7061 - val_accuracy: 0.7446\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    X_train_tmp, X_val_tmp, y_train_tmp, y_val_tmp = train_test_split(\n",
    "        np.concatenate((X_train, X_train_meta), axis=1), \n",
    "        y_train, \n",
    "        test_size=0.05)\n",
    "    \n",
    "    X_train_tmp, X_train_tmp_meta = X_train_tmp[:,:100], X_train_tmp[:,100:]\n",
    "    X_val_tmp, X_val_tmp_meta = X_val_tmp[:,:100], X_val_tmp[:,100:]\n",
    "\n",
    "    \n",
    "    model = model_1.get_model(embedding_matrix, maxlen, max_features, embed_size, len(features))\n",
    "    adam = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    print(\"##################################################\")\n",
    "    print(f'Model 1, iteration {i}')\n",
    "    \n",
    "    callback_earlystop = tf.keras.callbacks.EarlyStopping(patience=5,\n",
    "                                               monitor='val_loss',\n",
    "                                               mode='auto',\n",
    "                                               restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    model.fit([X_train_tmp, X_train_tmp_meta], y_train_tmp, epochs = 20, batch_size=256,\n",
    "              callbacks=[callback_earlystop], validation_data=([X_val_tmp, X_val_tmp_meta], y_val_tmp))\n",
    "    \n",
    "    y_hat_val = model.predict([X_val, X_val_meta])\n",
    "    y_hat_val.tofile(f'./results/Model_1_{i}_val_cls', dty)\n",
    "    \n",
    "    y_hat_test = model.predict([X_test, X_test_meta])\n",
    "    y_hat_test.tofile(f'./results/Model_1_{i}_test_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.tofile('./results/y_val_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1558420e111c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ml-nlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml-nlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "accuracy_score(y_hat_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_hat_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.argmax(y_hat_val, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_hat_val, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h = np.zeros_like(y_hat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    y_hat_tmp = np.fromfile(f'./results/Model_1_{i}_val_cls', dtype='float32').reshape(-1, 3)\n",
    "    \n",
    "    y_hat_tmp_cat = to_categorical(np.argmax(y_hat_tmp, axis=1))\n",
    "    \n",
    "    y_h = y_h + y_hat_tmp_cat\n",
    "\n",
    "y_h = y_h + np.random.normal(size=y_h.shape)/1e4    \n",
    "\n",
    "accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_h, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_tmp = np.fromfile(f'./results/Model_1_9_val_cls', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_tmp.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_tmp = np.fromfile(f'./results/Model_1_0_val_cls', dtype='float32').reshape(-1, 3)\n",
    "np.random.normal(size=y_hat_tmp.shape)/1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cf290153e199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-nlp)",
   "language": "python",
   "name": "ml-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
