{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data/Train.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ignore_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-569e2e20d883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opinion'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opinion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'&#039;ll'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' will'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'opinion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ml-nlp/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ignore_index'"
     ]
    }
   ],
   "source": [
    "dataset['opinion'].str.replace('\"', '').str.lower().str.replace('&#039;ve', ' have').str.replace('&#039;s', 'is')\n",
    "\n",
    "dataset['opinion'] = dataset['opinion'].str.lower()\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('\"', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace(',', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('.', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('!', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace(':', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;ve', ' have')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;s', ' is')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;t', ' not')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;m', ' am')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;ll', ' will')\n",
    "\n",
    "dataset = dataset.drop_duplicates('opinion').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(dataset, test_size=0.1, random_state=2137)\n",
    "\n",
    "X_train = train_df.opinion.values\n",
    "X_val = val_df.opinion.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './GoogleNews-vectors-negative300.bin.gz' \n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 25000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 75 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.opinion.values\n",
    "X_val = val_df.opinion.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  9  8 ...  6  3 10]\n"
     ]
    }
   ],
   "source": [
    "y_train = train_df['rate'].values\n",
    "y_val = val_df['rate'].values\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalMaxPool1D, Bidirectional, GRU, Embedding, Input, LSTM, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 75)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 75, 300)           7500000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 75, 300)           1200      \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 75, 256)           439296    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,960,081\n",
      "Trainable params: 7,958,969\n",
      "Non-trainable params: 1,112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((max_features, embed_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[i, :] = word2vec.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "x = BatchNormalization()(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "# x = GlobalMaxPool1D()(x)\n",
    "x = Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dense(32, activation=\"tanh\", kernel_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, kernel_regularizer=regularizers.l2(0.1))(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=adam,\n",
    "              metrics=['mse'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96733 samples, validate on 10749 samples\n",
      "Epoch 1/5\n",
      "96733/96733 [==============================] - 98s 1ms/sample - loss: 9.4405 - mse: 6.3469 - val_loss: 5.5526 - val_mse: 4.8161\n",
      "Epoch 2/5\n",
      "96733/96733 [==============================] - 96s 997us/sample - loss: 4.6803 - mse: 4.0151 - val_loss: 5.1686 - val_mse: 4.5600\n",
      "Epoch 3/5\n",
      "96733/96733 [==============================] - 94s 967us/sample - loss: 3.7263 - mse: 3.1207 - val_loss: 5.2453 - val_mse: 4.6755\n",
      "Epoch 4/5\n",
      "96733/96733 [==============================] - 94s 973us/sample - loss: 3.0204 - mse: 2.4371 - val_loss: 5.1277 - val_mse: 4.5542\n",
      "Epoch 5/5\n",
      "96733/96733 [==============================] - 94s 973us/sample - loss: 2.5543 - mse: 1.9943 - val_loss: 5.2194 - val_mse: 4.6679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83b85ed050>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 5, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.4701395],\n",
       "       [9.48248  ],\n",
       "       [8.765604 ],\n",
       "       ...,\n",
       "       [8.560794 ],\n",
       "       [9.01858  ],\n",
       "       [8.604201 ]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_cat = np.clip(np.rint(y_hat.flatten()), 1, 10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 9, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.771792724904643"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_hat_cat, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-nlp)",
   "language": "python",
   "name": "ml-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
