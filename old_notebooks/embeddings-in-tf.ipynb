{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embeddings(file_path, EMBEDDING_DIM, VOCAB_SIZE, word2idx):\n",
    "    # 1.load in pre-trained word vectors     #feature vector for each word\n",
    "    print(\"graph in function\",tf.get_default_graph())   \n",
    "    print('Loading word vectors...')\n",
    "    word2vec = {}\n",
    "    with open(os.path.join(file_path+'.%sd.txt' % EMBEDDING_DIM),  errors='ignore', encoding='utf8') as f:\n",
    "        # is just a space-separated text file in the format:\n",
    "        # word vec[0] vec[1] vec[2] ...\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vec\n",
    "\n",
    "        print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "        # 2.prepare embedding matrix\n",
    "        print('Filling pre-trained embeddings...')\n",
    "        num_words = VOCAB_SIZE\n",
    "        # initialization by zeros\n",
    "        embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "        for word, i in word2idx.items():\n",
    "            if i < VOCAB_SIZE:\n",
    "                embedding_vector = word2vec.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all zeros.\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./glove.6B\"\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./glove.6B.100d.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path+'.%sd.txt' % EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "\n",
    "with open(os.path.join(file_path+'.%sd.txt' % EMBEDDING_DIM),  errors='ignore', encoding='utf8') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word, w2v, embedding_dim):\n",
    "    vec = np.zeros(embedding_dim)\n",
    "    try:\n",
    "        vec = w2v[word]\n",
    "    except:\n",
    "        pass\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.0553e-01, -5.0886e-02, -1.5461e-01, -1.2327e-01,  6.6270e-01,\n",
       "       -2.8506e-01, -6.8844e-01,  4.9135e-01, -6.8924e-01,  3.8926e-01,\n",
       "        1.4359e-01, -4.8802e-01,  1.5746e-01,  8.3178e-01, -2.7923e-01,\n",
       "        9.4755e-03, -1.1207e-01, -5.2099e-01, -3.7159e-01, -3.7951e-01,\n",
       "        5.0083e-01, -3.4160e-01,  4.8098e-01, -1.1453e+00,  4.5958e-01,\n",
       "       -6.5640e-01,  4.3018e-01, -4.2527e-01,  2.3089e-01,  7.8911e-01,\n",
       "       -7.5434e-01,  1.0830e-01, -1.8071e-01, -5.5543e-04, -4.1071e-01,\n",
       "        8.6157e-01,  5.3711e-02,  2.4208e-01, -2.6254e-01, -3.0915e-01,\n",
       "       -2.9787e-01, -5.0758e-01, -2.9940e-01, -3.0442e-01,  7.3099e-01,\n",
       "        1.4165e-01,  1.0339e-01, -2.9659e-01,  9.9400e-01, -4.1594e-01,\n",
       "        3.8918e-01,  9.3532e-02,  1.0815e+00,  7.1774e-01, -1.1604e+00,\n",
       "       -3.0277e+00, -9.2490e-01, -8.8455e-02,  6.1408e-01, -2.5770e-01,\n",
       "       -2.6942e-01,  4.4647e-01, -8.3637e-01,  7.2481e-02,  3.0968e-02,\n",
       "       -2.5574e-01, -2.4832e-01,  4.5399e-01,  6.7532e-01, -7.0256e-02,\n",
       "       -1.2594e+00, -3.4572e-01, -4.8970e-01, -5.9699e-01, -1.0352e-01,\n",
       "       -1.2619e+00,  1.2068e+00, -1.4687e-01, -1.2883e+00, -7.5232e-01,\n",
       "        6.5286e-01,  3.8178e-01,  6.6523e-01,  3.1343e-01, -9.5396e-01,\n",
       "       -6.3114e-01, -2.3032e-01, -1.6693e-01, -1.4885e-02,  7.4412e-01,\n",
       "       -1.7945e-01, -5.0046e-01,  8.2233e-01,  2.3110e-01, -7.6910e-01,\n",
       "        9.4608e-01,  4.3809e-01,  4.1467e-01,  3.4641e-01,  1.9886e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector('london', word2vec, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data/Train.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>condition</th>\n",
       "      <th>opinion</th>\n",
       "      <th>rate</th>\n",
       "      <th>rate1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zegerid</td>\n",
       "      <td>GERD</td>\n",
       "      <td>\"Using it as a replacement for Nexium, since i...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethosuximide</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>\"This medicine is very good at controlling me ...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tri-Sprintec</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I just started taking Tri Sprintec after my l...</td>\n",
       "      <td>9</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Levaquin</td>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>\"This medicine made me feel absolutely horribl...</td>\n",
       "      <td>5</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Methylphenidate</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"I&amp;#039;ve been taking Concerta since 2003. Fo...</td>\n",
       "      <td>9</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>Prozac</td>\n",
       "      <td>Anxiety and Stress</td>\n",
       "      <td>\"I was diagnosed with generalized anxiety and ...</td>\n",
       "      <td>9</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>Scopolamine</td>\n",
       "      <td>Motion Sickness</td>\n",
       "      <td>\"Works great for motion sickness.  Much more e...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>Epiduo</td>\n",
       "      <td>Acne</td>\n",
       "      <td>\"I&amp;#039;ve been using Epiduo for about 8 month...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>Phillips' Milk of Magnesia</td>\n",
       "      <td>Constipation</td>\n",
       "      <td>\"Taking pain killers for shingles I was consta...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>Suprep Bowel Prep Kit</td>\n",
       "      <td>Bowel Preparation</td>\n",
       "      <td>\"It&amp;#039;s not that bad,  the taste could be m...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name           condition  \\\n",
       "0                          Zegerid                GERD   \n",
       "1                     Ethosuximide            Seizures   \n",
       "2                     Tri-Sprintec       Birth Control   \n",
       "3                         Levaquin           Pneumonia   \n",
       "4                  Methylphenidate                ADHD   \n",
       "...                            ...                 ...   \n",
       "149995                      Prozac  Anxiety and Stress   \n",
       "149996                 Scopolamine     Motion Sickness   \n",
       "149997                      Epiduo                Acne   \n",
       "149998  Phillips' Milk of Magnesia        Constipation   \n",
       "149999       Suprep Bowel Prep Kit   Bowel Preparation   \n",
       "\n",
       "                                                  opinion  rate   rate1  \n",
       "0       \"Using it as a replacement for Nexium, since i...    10    high  \n",
       "1       \"This medicine is very good at controlling me ...    10    high  \n",
       "2       \"I just started taking Tri Sprintec after my l...     9    high  \n",
       "3       \"This medicine made me feel absolutely horribl...     5  medium  \n",
       "4       \"I&#039;ve been taking Concerta since 2003. Fo...     9    high  \n",
       "...                                                   ...   ...     ...  \n",
       "149995  \"I was diagnosed with generalized anxiety and ...     9    high  \n",
       "149996  \"Works great for motion sickness.  Much more e...    10    high  \n",
       "149997  \"I&#039;ve been using Epiduo for about 8 month...    10    high  \n",
       "149998  \"Taking pain killers for shingles I was consta...    10    high  \n",
       "149999  \"It&#039;s not that bad,  the taste could be m...    10    high  \n",
       "\n",
       "[150000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['opinion'].str.replace('\"', '').str.lower().str.replace('&#039;ve', ' have').str.replace('&#039;s', 'is')\n",
    "\n",
    "dataset['opinion'] = dataset['opinion'].str.lower()\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('\"', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace(',', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('.', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('!', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace(':', '')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;ve', ' have')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;s', ' is')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;t', ' not')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;m', ' am')\n",
    "dataset['opinion'] = dataset['opinion'].str.replace('&#039;ll', ' will')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_opinion = dataset['opinion'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>condition</th>\n",
       "      <th>opinion</th>\n",
       "      <th>rate</th>\n",
       "      <th>rate1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zegerid</td>\n",
       "      <td>GERD</td>\n",
       "      <td>using it as a replacement for nexium since ins...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethosuximide</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>this medicine is very good at controlling me s...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tri-Sprintec</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>i just started taking tri sprintec after my la...</td>\n",
       "      <td>9</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Levaquin</td>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>this medicine made me feel absolutely horrible...</td>\n",
       "      <td>5</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Methylphenidate</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>i have been taking concerta since 2003 for me ...</td>\n",
       "      <td>9</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>Prozac</td>\n",
       "      <td>Anxiety and Stress</td>\n",
       "      <td>i was diagnosed with generalized anxiety and p...</td>\n",
       "      <td>9</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>Scopolamine</td>\n",
       "      <td>Motion Sickness</td>\n",
       "      <td>works great for motion sickness  much more eff...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>Epiduo</td>\n",
       "      <td>Acne</td>\n",
       "      <td>i have been using epiduo for about 8 months no...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>Phillips' Milk of Magnesia</td>\n",
       "      <td>Constipation</td>\n",
       "      <td>taking pain killers for shingles i was constan...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>Suprep Bowel Prep Kit</td>\n",
       "      <td>Bowel Preparation</td>\n",
       "      <td>it is not that bad  the taste could be more fl...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name           condition  \\\n",
       "0                          Zegerid                GERD   \n",
       "1                     Ethosuximide            Seizures   \n",
       "2                     Tri-Sprintec       Birth Control   \n",
       "3                         Levaquin           Pneumonia   \n",
       "4                  Methylphenidate                ADHD   \n",
       "...                            ...                 ...   \n",
       "149995                      Prozac  Anxiety and Stress   \n",
       "149996                 Scopolamine     Motion Sickness   \n",
       "149997                      Epiduo                Acne   \n",
       "149998  Phillips' Milk of Magnesia        Constipation   \n",
       "149999       Suprep Bowel Prep Kit   Bowel Preparation   \n",
       "\n",
       "                                                  opinion  rate   rate1  \n",
       "0       using it as a replacement for nexium since ins...    10    high  \n",
       "1       this medicine is very good at controlling me s...    10    high  \n",
       "2       i just started taking tri sprintec after my la...     9    high  \n",
       "3       this medicine made me feel absolutely horrible...     5  medium  \n",
       "4       i have been taking concerta since 2003 for me ...     9    high  \n",
       "...                                                   ...   ...     ...  \n",
       "149995  i was diagnosed with generalized anxiety and p...     9    high  \n",
       "149996  works great for motion sickness  much more eff...    10    high  \n",
       "149997  i have been using epiduo for about 8 months no...    10    high  \n",
       "149998  taking pain killers for shingles i was constan...    10    high  \n",
       "149999  it is not that bad  the taste could be more fl...    10    high  \n",
       "\n",
       "[150000 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>condition</th>\n",
       "      <th>opinion</th>\n",
       "      <th>rate</th>\n",
       "      <th>rate1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zegerid</td>\n",
       "      <td>GERD</td>\n",
       "      <td>using it as a replacement for nexium since ins...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethosuximide</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>this medicine is very good at controlling me s...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tri-Sprintec</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>i just started taking tri sprintec after my la...</td>\n",
       "      <td>9</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Levaquin</td>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>this medicine made me feel absolutely horrible...</td>\n",
       "      <td>5</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Methylphenidate</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>i have been taking concerta since 2003 for me ...</td>\n",
       "      <td>9</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149989</th>\n",
       "      <td>Tri-Lo-Sprintec</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>i had use bc before i stoped it because i was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149990</th>\n",
       "      <td>Ethinyl estradiol / levonorgestrel</td>\n",
       "      <td>Abnormal Uterine Bleeding</td>\n",
       "      <td>i didn not go on this pill as a birth control ...</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149992</th>\n",
       "      <td>Ethinyl estradiol / norethindrone</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>i have read almost all the reviews and i have ...</td>\n",
       "      <td>1</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149993</th>\n",
       "      <td>Naproxen</td>\n",
       "      <td>Sciatica</td>\n",
       "      <td>i have had very big problems been taking norco...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>Epiduo</td>\n",
       "      <td>Acne</td>\n",
       "      <td>i have been using epiduo for about 8 months no...</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107482 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name                  condition  \\\n",
       "0                                  Zegerid                       GERD   \n",
       "1                             Ethosuximide                   Seizures   \n",
       "2                             Tri-Sprintec              Birth Control   \n",
       "3                                 Levaquin                  Pneumonia   \n",
       "4                          Methylphenidate                       ADHD   \n",
       "...                                    ...                        ...   \n",
       "149989                     Tri-Lo-Sprintec              Birth Control   \n",
       "149990  Ethinyl estradiol / levonorgestrel  Abnormal Uterine Bleeding   \n",
       "149992   Ethinyl estradiol / norethindrone              Birth Control   \n",
       "149993                            Naproxen                   Sciatica   \n",
       "149997                              Epiduo                       Acne   \n",
       "\n",
       "                                                  opinion  rate   rate1  \n",
       "0       using it as a replacement for nexium since ins...    10    high  \n",
       "1       this medicine is very good at controlling me s...    10    high  \n",
       "2       i just started taking tri sprintec after my la...     9    high  \n",
       "3       this medicine made me feel absolutely horrible...     5  medium  \n",
       "4       i have been taking concerta since 2003 for me ...     9    high  \n",
       "...                                                   ...   ...     ...  \n",
       "149989  i had use bc before i stoped it because i was ...     1     low  \n",
       "149990  i didn not go on this pill as a birth control ...     6  medium  \n",
       "149992  i have read almost all the reviews and i have ...     1     low  \n",
       "149993  i have had very big problems been taking norco...    10    high  \n",
       "149997  i have been using epiduo for about 8 months no...    10    high  \n",
       "\n",
       "[107482 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop_duplicates('opinion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_mean(opinion: str, w2v, embedding_dim):\n",
    "    words = opinion.split()\n",
    "    vec_sum = np.zeros(embedding_dim)\n",
    "    number_of_words = 0\n",
    "    for word in words:\n",
    "        vec = get_vector(word, w2v, embedding_dim)\n",
    "        if np.linalg.norm(vec-np.zeros(embedding_dim)) > 1e-5:\n",
    "            number_of_words += 1\n",
    "        vec_sum = vec_sum + vec\n",
    "    return vec_sum / number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15427511,  0.12707239,  0.35868712, -0.18218694,  0.01455108,\n",
       "        0.21512224, -0.07018973,  0.2298978 , -0.06927736,  0.04388645,\n",
       "       -0.01925125, -0.04865904,  0.14613595,  0.00425707,  0.21716357,\n",
       "       -0.26520777,  0.18001961, -0.03260566, -0.22419181,  0.16553172,\n",
       "        0.31610323, -0.00798679,  0.14207322,  0.10039605,  0.23661318,\n",
       "       -0.0170113 , -0.20278803, -0.48539128, -0.01207295, -0.1538183 ,\n",
       "       -0.17432093,  0.46031325, -0.06569939, -0.02854962, -0.03786707,\n",
       "        0.3628991 ,  0.01092459,  0.21116129,  0.00590539, -0.13569826,\n",
       "       -0.26142451, -0.14836966,  0.20192877, -0.32625865,  0.02002101,\n",
       "        0.10060166,  0.30492613, -0.31180021, -0.05516054, -0.70789443,\n",
       "        0.04435397, -0.13992881,  0.18456231,  1.03101926, -0.27566989,\n",
       "       -2.3161382 , -0.05861487, -0.25814567,  1.4924487 ,  0.35687561,\n",
       "       -0.02166272,  0.65744   , -0.19823895,  0.1483947 ,  0.65397533,\n",
       "       -0.024082  ,  0.45192134,  0.15319641,  0.20289282, -0.20543327,\n",
       "       -0.1432975 , -0.29548525, -0.09358538, -0.19992264,  0.13493534,\n",
       "        0.05777595, -0.22045413, -0.05161074, -0.77297782, -0.02083021,\n",
       "        0.53654561, -0.01144724, -0.44901205,  0.15793492, -1.24970869,\n",
       "       -0.11480312,  0.19688773, -0.03833446, -0.22098566, -0.35788205,\n",
       "        0.06241672, -0.12218934, -0.16233336,  0.05827845, -0.36453493,\n",
       "       -0.01081112, -0.08456088, -0.24662669,  0.46183114,  0.09403964])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_mean(sample_opinion, word2vec, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectors = dataset['opinion'].apply(opinion_mean,args=(word2vec, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(X_vectors.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 100)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['rate'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = keras.Sequential()\n",
    "\n",
    "simple_cnn.add(layers.Dense(128, activation='relu'))\n",
    "simple_cnn.add(layers.Dense(128, activation='relu'))\n",
    "simple_cnn.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4454/4454 [==============================] - 5s 1ms/step - loss: nan - accuracy: 0.1505 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 2/30\n",
      "4454/4454 [==============================] - 4s 855us/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 3/30\n",
      "4454/4454 [==============================] - 5s 1ms/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 4/30\n",
      "4454/4454 [==============================] - 6s 1ms/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 5/30\n",
      "4454/4454 [==============================] - 5s 1ms/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 6/30\n",
      "4454/4454 [==============================] - 4s 888us/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 7/30\n",
      "4454/4454 [==============================] - 4s 849us/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 8/30\n",
      "4454/4454 [==============================] - 4s 853us/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 9/30\n",
      "4454/4454 [==============================] - 4s 915us/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 10/30\n",
      "4454/4454 [==============================] - 4s 838us/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 11/30\n",
      "4454/4454 [==============================] - 4s 953us/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 12/30\n",
      "4454/4454 [==============================] - 4s 969us/step - loss: nan - accuracy: 0.1344 - val_loss: nan - val_accuracy: 0.1345\n",
      "Epoch 13/30\n",
      "2977/4454 [===================>..........] - ETA: 1s - loss: nan - accuracy: 0.1338"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-f8fbca283aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m   return pack_sequence_as(\n\u001b[0m\u001b[1;32m    617\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m   \"\"\"\n\u001b[0;32m--> 552\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     final_index, packed = _packed_nest_with_indices(structure, flat_sequence,\n\u001b[0m\u001b[1;32m    506\u001b[0m                                                     0, is_seq, sequence_fn)\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_packed_nest_with_indices\u001b[0;34m(structure, flat, index, is_seq, sequence_fn)\u001b[0m\n\u001b[1;32m    465\u001b[0m   \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m   \u001b[0msequence_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sequence_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_yield_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m       new_index, child = _packed_nest_with_indices(s, flat, index, is_seq,\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_yield_value\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_yield_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_yield_sorted_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_yield_sorted_items\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder\u001b[0m \u001b[0mof\u001b[0m \u001b[0msorted\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \"\"\"\n\u001b[0;32m--> 211\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_collections_abc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;31m# Iterate through dictionaries in a deterministic order by sorting the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# keys. Notice this means that we ignore the original order of `OrderedDict`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-new/lib/python3.8/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simple_cnn.fit(X_train, y_train, epochs=30, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-new)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
